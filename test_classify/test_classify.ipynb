{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import random\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"test_classification_data_with_date.csv\", index_col = False)\n",
    "df = pd.read_csv(\"test_classification_data.csv\", index_col = False)\n",
    "# df = df.drop(df[(df.Class == 'H')].index).drop('date', axis = 1)\n",
    "df = df.drop('date', axis = 1)\n",
    "X = df.drop('Class', axis = 1)\n",
    "Y = df['Class']\n",
    "\n",
    "kf = KFold(n_splits = 3)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "plt.figure(figsize=(80,15))\n",
    "for i in range (X.shape[0]):\n",
    "    if Y.iloc[i] == 2:\n",
    "        plt.plot(X.iloc[i], color = 'r')\n",
    "    if Y.iloc[i] == 1:\n",
    "        plt.plot(X.iloc[i], color = 'b')\n",
    "    if Y.iloc[i] == 0:\n",
    "        plt.plot(X.iloc[i], color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, 15):\n",
    "    print(\"kernel: poly, degree: \", i)\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        SVClassifier = SVC(kernel = 'poly', degree = i)\n",
    "        SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "        y_train_pred = SVClassifier.predict(X_train)\n",
    "        y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    #     print(confusion_matrix(y_train_pred, Y_train))\n",
    "    #     print(classification_report(y_train_pred, Y_train))\n",
    "    #     print(confusion_matrix(y_test_pred, Y_test))\n",
    "        train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "        test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "    print(\"average training accuracy: \", train_acc / 5)\n",
    "    print(\"average testing accuracy: \", test_acc / 5)\n",
    "    print()\n",
    "\n",
    "print(\"kernel: linear\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    SVClassifier = SVC(kernel = 'linear')\n",
    "    SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "    y_train_pred = SVClassifier.predict(X_train)\n",
    "    y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()\n",
    "\n",
    "print(\"kernel: rbf\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    SVClassifier = SVC()\n",
    "    SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "    y_train_pred = SVClassifier.predict(X_train)\n",
    "    y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()\n",
    "\n",
    "print(\"kernel: sigmoid\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    SVClassifier = SVC(kernel = 'sigmoid')\n",
    "    SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "    y_train_pred = SVClassifier.predict(X_train)\n",
    "    y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    logreg_clf = LogisticRegression()\n",
    "    logreg_clf.fit(X_train, Y_train)\n",
    "    logreg_train_pred = logreg_clf.predict(X_train)\n",
    "    logreg_test_pred = logreg_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(logreg_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(logreg_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    tree_clf = DecisionTreeClassifier()\n",
    "    tree_clf.fit(X_train, Y_train)\n",
    "    tree_train_pred = tree_clf.predict(X_train)\n",
    "    tree_test_pred = tree_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(tree_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(tree_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"K Nearest Neighbor\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    knn_clf.fit(X_train, Y_train)\n",
    "    knn_train_pred = knn_clf.predict(X_train)\n",
    "    knn_test_pred = knn_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(knn_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(knn_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Discriminant Analysis\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    lda_clf.fit(X_train, Y_train)\n",
    "    lda_train_pred = lda_clf.predict(X_train)\n",
    "    lda_test_pred = lda_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(lda_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(lda_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gaussian Naive Bayes\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf.fit(X_train, Y_train)\n",
    "    gnb_train_pred = gnb_clf.predict(X_train)\n",
    "    gnb_test_pred = gnb_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(gnb_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(gnb_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest - gini\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    rf_clf = RandomForestClassifier()\n",
    "    rf_clf.fit(X_train, Y_train)\n",
    "    rf_train_pred = rf_clf.predict(X_train)\n",
    "    rf_test_pred = rf_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(rf_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(rf_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest - entropy\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    rf2_clf = RandomForestClassifier(criterion = 'entropy')\n",
    "    rf2_clf.fit(X_train, Y_train)\n",
    "    rf2_train_pred = rf2_clf.predict(X_train)\n",
    "    rf2_test_pred = rf2_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(rf2_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(rf2_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# MNIST dataset parameters\n",
    "num_classes = 2\n",
    "num_features = 50\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 20000\n",
    "batch_size = 256\n",
    "display_steps = 250\n",
    "\n",
    "# load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(20000).batch(batch_size).prefetch(1)\n",
    "\n",
    "# Weight of shape [784, 10], the 28*28 image features, and total number of classes.\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "# Bias of shape [10], the total number of classes.\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
    "\n",
    "# logistic regression (Wx + b)\n",
    "def logistic_regression(x):\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    # ****************\n",
    "    # softmax is a generalization of sigmoid function\n",
    "    # sigmoid function is specifiically for binary classification\n",
    "    # ****************\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy_LogReg(y_pred, y_true):\n",
    "    # Encode label to a one hot vector.\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    # Compute cross-entropy.\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred),1))\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy_LogReg(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "def run_optimization_LogReg(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = logistic_regression(x)\n",
    "        loss = cross_entropy_LogReg(pred, y)\n",
    "        \n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate (train_data.take(training_steps), 1):\n",
    "    run_optimization_LogReg(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_steps == 0:\n",
    "        pred = logistic_regression(batch_x)\n",
    "        loss = cross_entropy_LogReg(pred, batch_y)\n",
    "        acc = accuracy_LogReg(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "        \n",
    "# Test model on validation set.\n",
    "pred = logistic_regression(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy_LogReg(pred, y_test))\n",
    "\n",
    "# # Predict 5 images from validation set.\n",
    "# n_images = 5\n",
    "# test_images = random.choices(x_test, k = n_images)\n",
    "# predictions = logistic_regression(test_images)\n",
    "\n",
    "# # Display image and model prediction.\n",
    "# for i in range(n_images):\n",
    "# #     plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "# #     plt.show()\n",
    "#     print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, loss: 0.595084, accuracy: 0.781250\n",
      "step: 250, loss: 0.075401, accuracy: 1.000000\n",
      "step: 500, loss: 0.026072, accuracy: 1.000000\n",
      "step: 750, loss: 0.014617, accuracy: 1.000000\n",
      "step: 1000, loss: 0.008735, accuracy: 1.000000\n",
      "step: 1250, loss: 0.006895, accuracy: 1.000000\n",
      "step: 1500, loss: 0.004996, accuracy: 1.000000\n",
      "step: 1750, loss: 0.004022, accuracy: 1.000000\n",
      "step: 2000, loss: 0.003328, accuracy: 1.000000\n",
      "Test Accuracy: 0.692308\n",
      "step: 5, loss: 0.489473, accuracy: 0.742188\n",
      "step: 250, loss: 0.006366, accuracy: 1.000000\n",
      "step: 500, loss: 0.003370, accuracy: 1.000000\n",
      "step: 750, loss: 0.002462, accuracy: 1.000000\n",
      "step: 1000, loss: 0.001629, accuracy: 1.000000\n",
      "step: 1250, loss: 0.001342, accuracy: 1.000000\n",
      "step: 1500, loss: 0.001028, accuracy: 1.000000\n",
      "step: 1750, loss: 0.001108, accuracy: 1.000000\n",
      "step: 2000, loss: 0.000972, accuracy: 1.000000\n",
      "Test Accuracy: 0.692308\n",
      "step: 5, loss: 0.888283, accuracy: 0.777344\n",
      "step: 250, loss: 0.012303, accuracy: 1.000000\n",
      "step: 500, loss: 0.006881, accuracy: 1.000000\n",
      "step: 750, loss: 0.004766, accuracy: 1.000000\n",
      "step: 1000, loss: 0.004184, accuracy: 1.000000\n",
      "step: 1250, loss: 0.002544, accuracy: 1.000000\n",
      "step: 1500, loss: 0.002968, accuracy: 1.000000\n",
      "step: 1750, loss: 0.002254, accuracy: 1.000000\n",
      "step: 2000, loss: 0.001732, accuracy: 1.000000\n",
      "Test Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "# MNIST dataset parameters\n",
    "num_classes = 2\n",
    "num_features = 50\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 2000\n",
    "batch_size = 256\n",
    "display_steps = 250\n",
    "\n",
    "# Network parameters\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 128\n",
    "\n",
    "# load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "# x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# # x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# # x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "# # Use tf.data API to shuffle and batch data.\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_data = train_data.repeat().shuffle(20000).batch(batch_size).prefetch(1)\n",
    "\n",
    "# Create NN model\n",
    "class Neural_Network(Model):\n",
    "    # Set layers\n",
    "    def __init__(self):\n",
    "        tf.keras.backend.clear_session()\n",
    "        super(Neural_Network, self).__init__();\n",
    "        # 1st fully connected hidden layer\n",
    "        self.fc1 = layers.Dense(n_hidden_1, activation = tf.nn.relu)\n",
    "        # 2nd fully connected hidden layer\n",
    "        self.fc2 = layers.Dense(n_hidden_2, activation = tf.nn.relu)\n",
    "        # Output layer\n",
    "        self.out = layers.Dense(num_classes)\n",
    "        \n",
    "    # Set forward pass\n",
    "    def call(self, x, is_training = False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build NN model\n",
    "neural_network = Neural_Network()\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy_NN(y_pred, y_true):\n",
    "    # Convert labels to int64 for tf cross-entropy function\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n",
    "    # Average loss across batch\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy_NN(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis = -1)\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Optimization process\n",
    "def run_optimization_NN(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = neural_network(x, is_training = True)\n",
    "        loss = cross_entropy_NN(pred, y)\n",
    "        \n",
    "    trainable_variables = neural_network.trainable_variables\n",
    "    \n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "#     x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "#     x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "    # Use tf.data API to shuffle and batch data.\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.repeat().shuffle(20000).batch(batch_size).prefetch(1)\n",
    "    for step, (batch_x, batch_y) in enumerate (train_data.take(training_steps), 1):\n",
    "        run_optimization_NN(batch_x, batch_y)\n",
    "\n",
    "        if step % display_steps == 0 or step == 5:\n",
    "            pred = neural_network(batch_x, is_training = True)\n",
    "            loss = cross_entropy_NN(pred, batch_y)\n",
    "            acc = accuracy_NN(pred, batch_y)\n",
    "            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "        \n",
    "    # Test model on validation set.\n",
    "    pred = neural_network(x_test, is_training = False)\n",
    "    print(\"Test Accuracy: %f\" % accuracy_NN(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# print(x_train)\n",
    "# print(x_test.shape)\n",
    "# print(X)\n",
    "# print(Y)\n",
    "# print(y_train)\n",
    "x = np.array(X, np.float32)\n",
    "pred = neural_network(x, is_training = False)\n",
    "print(accuracy_NN(pred, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "T = pd.read_csv(\"test_data.csv\", index_col = False)\n",
    "x_t = T.drop('Class', axis = 1)\n",
    "x_t = np.array(x_t, np.float32)\n",
    "y_t = T['Class']\n",
    "pred = neural_network(x_t, is_training = False)\n",
    "print(accuracy_NN(pred, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test_NN_1.0\\assets\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.models.save_model(neural_network, \"test_NN_1.0.h5\")\n",
    "neural_network.save('test_NN_1.0')\n",
    "new_model = tf.keras.models.load_model('test_NN_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"neural__network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  13056     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  258       \n",
      "=================================================================\n",
      "Total params: 46,210\n",
      "Trainable params: 46,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Variable 'neural__network/dense/kernel:0' shape=(50, 256) dtype=float32, numpy=\n",
      "array([[-0.02851551,  0.07126708, -0.08852716, ..., -0.11467671,\n",
      "        -0.00266498,  0.11319671],\n",
      "       [ 0.0846305 ,  0.00491714,  0.03590145, ..., -0.06061811,\n",
      "         0.05032223,  0.11623302],\n",
      "       [ 0.09948798, -0.03401306, -0.03179406, ..., -0.08054719,\n",
      "        -0.09883135, -0.08336414],\n",
      "       ...,\n",
      "       [-0.07208821, -0.03687027, -0.13702571, ..., -0.11132385,\n",
      "        -0.07099997,  0.03943052],\n",
      "       [-0.07284936,  0.04598966,  0.05797716, ...,  0.1315587 ,\n",
      "        -0.01768285, -0.02859458],\n",
      "       [-0.06654114,  0.07501666,  0.11923212, ..., -0.06710834,\n",
      "         0.02437128,  0.04271344]], dtype=float32)>, <tf.Variable 'neural__network/dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([-3.1032664e-04,  0.0000000e+00, -3.8989552e-03, -4.2558930e-04,\n",
      "       -6.1611703e-04,  5.5088622e-05, -6.3203339e-04, -1.7269418e-04,\n",
      "       -1.4548567e-04,  1.2143005e-03,  3.2820189e-04,  8.8664406e-04,\n",
      "        3.8065441e-04, -8.0393744e-04, -3.5357548e-04, -1.6109977e-03,\n",
      "       -5.2748731e-04,  2.1133740e-03, -1.7460350e-04,  2.7921001e-04,\n",
      "       -2.9086691e-04, -4.2454712e-04, -1.6426672e-03,  5.3371303e-04,\n",
      "       -6.4210537e-05,  0.0000000e+00, -6.3455035e-04, -1.3463253e-03,\n",
      "       -1.5018866e-03,  0.0000000e+00,  5.0157058e-04,  0.0000000e+00,\n",
      "        1.5143330e-03,  3.4613062e-03,  4.1363228e-04, -1.9629588e-03,\n",
      "       -2.9691060e-03, -2.5912768e-03,  4.1387396e-04, -3.6321592e-04,\n",
      "        2.9744234e-04,  0.0000000e+00, -7.8391895e-05,  1.2094239e-03,\n",
      "        0.0000000e+00,  5.7993887e-04,  1.1367166e-04,  0.0000000e+00,\n",
      "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.4422246e-03,\n",
      "        3.4636140e-03,  2.4999247e-04, -9.8502147e-04,  2.1743262e-03,\n",
      "        2.3976398e-04,  2.5914703e-04,  9.5220277e-04,  9.2435046e-04,\n",
      "       -5.4414076e-04,  0.0000000e+00, -3.1617642e-04, -1.3266613e-04,\n",
      "        7.8472309e-04,  3.9194769e-04, -4.2906040e-04, -1.9740590e-03,\n",
      "       -4.1702640e-04, -2.2370445e-03, -5.3850835e-04, -1.9963461e-03,\n",
      "       -1.5923855e-03, -7.2710338e-04,  1.7883719e-03, -1.4628288e-03,\n",
      "       -7.7410723e-04, -4.1111186e-03, -2.5783658e-03,  1.0684792e-04,\n",
      "        5.6360802e-04,  0.0000000e+00, -6.1105401e-04,  2.5743039e-03,\n",
      "        6.8345671e-06, -1.1219741e-03, -8.1550199e-05, -1.0030973e-03,\n",
      "       -1.6772801e-04, -8.1091333e-04, -6.2621041e-04, -1.5471900e-03,\n",
      "        0.0000000e+00,  2.3232175e-04,  1.0321341e-03, -1.6872413e-03,\n",
      "       -1.3993820e-03, -3.9810457e-04, -1.5144662e-03,  0.0000000e+00,\n",
      "       -1.0185919e-03,  0.0000000e+00, -4.5986348e-04,  6.2989542e-04,\n",
      "        1.0589488e-03, -7.2909275e-04,  0.0000000e+00, -3.1091389e-04,\n",
      "        7.6832093e-06, -1.0778098e-04,  1.6059555e-04,  0.0000000e+00,\n",
      "        2.2618953e-04, -7.4614660e-04,  0.0000000e+00, -4.5794723e-04,\n",
      "       -6.6826462e-05, -8.8523421e-04,  1.4348554e-05,  3.0098361e-04,\n",
      "       -1.0201192e-03, -1.3693817e-03,  7.1562408e-04,  8.1569381e-04,\n",
      "       -4.7982295e-04, -4.9734407e-04,  3.6540923e-03,  0.0000000e+00,\n",
      "       -2.0607344e-03, -1.6568655e-03, -1.2141570e-03,  0.0000000e+00,\n",
      "       -1.4286807e-03,  0.0000000e+00,  1.2895933e-03, -1.1964507e-03,\n",
      "        9.0366404e-04,  8.4550283e-04,  0.0000000e+00, -2.0864724e-04,\n",
      "        1.3630004e-03, -5.0042872e-04,  1.8271902e-03,  1.8342577e-03,\n",
      "        8.7799807e-04,  0.0000000e+00,  0.0000000e+00, -1.3824101e-03,\n",
      "       -4.0774126e-04, -1.4249207e-03, -7.5589604e-04, -2.4917205e-03,\n",
      "       -6.2553113e-06, -3.0586898e-04, -1.7102590e-03, -2.1981513e-03,\n",
      "        2.1609075e-04, -1.2625692e-03,  1.3017094e-03, -3.7107791e-04,\n",
      "        6.8457529e-04, -4.2952105e-04, -8.6295237e-05, -2.0649227e-04,\n",
      "       -7.2979834e-04, -2.5843541e-04, -5.5248267e-04, -2.5035904e-04,\n",
      "        3.0411088e-03, -9.2353305e-04,  0.0000000e+00, -2.6186295e-03,\n",
      "       -2.2184229e-03, -1.7077109e-03,  2.2718594e-03,  5.7229623e-05,\n",
      "        1.6904683e-04, -6.8060507e-04, -5.3747126e-04,  4.1998806e-04,\n",
      "        8.5625710e-04, -3.0280240e-03,  0.0000000e+00,  0.0000000e+00,\n",
      "       -3.2314714e-03, -1.2373321e-03,  0.0000000e+00,  3.6394162e-04,\n",
      "       -2.0649734e-04,  2.7802784e-03, -1.6743616e-03, -2.9480169e-03,\n",
      "        0.0000000e+00, -1.0280262e-03, -3.3728385e-04, -8.6861424e-04,\n",
      "       -2.1510596e-04, -3.7391168e-05, -2.6877236e-03, -9.4382936e-05,\n",
      "       -1.9447813e-03,  4.9071602e-04,  5.4311065e-04,  1.2503838e-03,\n",
      "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.6950100e-03,\n",
      "        0.0000000e+00, -7.9024275e-04,  5.2687945e-03, -3.7597211e-03,\n",
      "       -4.4960179e-04,  2.3823715e-04,  1.9081010e-04,  2.1439414e-03,\n",
      "        2.9890440e-04, -7.8895449e-04, -3.3799041e-04,  0.0000000e+00,\n",
      "        0.0000000e+00, -1.0145361e-03, -2.7517203e-04,  8.1037084e-04,\n",
      "        2.0351585e-03, -7.0992263e-04, -1.9933351e-03,  6.6985574e-04,\n",
      "       -1.2051809e-03, -3.9149537e-03,  0.0000000e+00, -2.6291916e-03,\n",
      "        1.3771933e-04,  2.3201338e-03,  7.6497049e-04,  1.1562912e-03,\n",
      "        0.0000000e+00, -1.6715190e-03,  8.9170143e-04,  0.0000000e+00,\n",
      "       -5.3166546e-04, -5.3143507e-04, -2.0413429e-03, -2.4186027e-04,\n",
      "       -1.6915321e-03,  0.0000000e+00,  0.0000000e+00, -2.6302699e-03,\n",
      "       -1.0783096e-04, -3.7495431e-04, -1.5816782e-03, -1.7085889e-03,\n",
      "       -6.6654681e-04,  2.4476149e-03,  0.0000000e+00, -1.2026425e-03],\n",
      "      dtype=float32)>, <tf.Variable 'neural__network/dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[ 0.08482301, -0.04523839, -0.00731255, ..., -0.04236151,\n",
      "        -0.08872224, -0.0440079 ],\n",
      "       [-0.00164181, -0.11169836,  0.00998205, ..., -0.00088397,\n",
      "        -0.06225273,  0.05136871],\n",
      "       [-0.10532453, -0.10049853,  0.09629902, ..., -0.06967016,\n",
      "        -0.03007203, -0.035457  ],\n",
      "       ...,\n",
      "       [-0.0528041 ,  0.07239781, -0.10972404, ..., -0.01819572,\n",
      "        -0.05864846,  0.01425752],\n",
      "       [-0.07494277, -0.04494593,  0.11928356, ..., -0.10426167,\n",
      "         0.01304817,  0.0927023 ],\n",
      "       [ 0.05661979,  0.01177583, -0.12022825, ...,  0.03165717,\n",
      "         0.03025665, -0.0569267 ]], dtype=float32)>, <tf.Variable 'neural__network/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([ 7.5274607e-04, -1.5472341e-03, -1.3593653e-04, -5.1378470e-04,\n",
      "        2.5904592e-04, -3.0496824e-04,  1.1128236e-03,  0.0000000e+00,\n",
      "       -1.6103942e-04, -1.1524729e-03, -1.4207469e-03, -3.8724614e-04,\n",
      "       -2.2767878e-03,  1.6673638e-03, -1.1200606e-03, -1.5476857e-03,\n",
      "       -5.4757285e-04, -4.2456129e-04,  1.5213838e-03,  6.9154846e-04,\n",
      "       -8.5249334e-04, -8.5067863e-05,  5.7919842e-05, -2.2721132e-03,\n",
      "        2.3703734e-04,  4.7142647e-05,  4.1844518e-04, -2.4498226e-03,\n",
      "        7.4527517e-04,  3.8432510e-04, -1.1935751e-03, -3.2856958e-03,\n",
      "        6.8757241e-04, -4.9874245e-04, -1.3702854e-03, -1.5756626e-03,\n",
      "        2.2118313e-04,  3.8713080e-04,  3.1695451e-04, -1.0825275e-03,\n",
      "        1.6970836e-03,  2.0749241e-03, -4.0053469e-04, -7.8067220e-05,\n",
      "       -1.0929982e-03, -1.3822602e-03, -1.2509441e-03,  1.4301415e-06,\n",
      "       -1.2862995e-03, -1.0604974e-03,  1.3230214e-03, -1.8178773e-03,\n",
      "        5.1536124e-05,  3.1565013e-04, -2.1974652e-03, -1.7178076e-03,\n",
      "       -3.2295162e-04, -8.2003302e-05,  1.7652134e-03,  8.8276494e-05,\n",
      "        5.3624657e-04, -6.2717416e-05, -1.8716623e-03, -1.2840409e-04,\n",
      "        5.4590515e-04,  1.3233136e-04,  1.4755850e-04,  1.9157995e-03,\n",
      "        1.2213249e-04, -7.5465371e-04, -2.6845632e-04,  9.1867719e-04,\n",
      "       -2.3505345e-03, -9.5989613e-04,  4.6259258e-03, -4.7610971e-04,\n",
      "       -7.9470908e-04, -1.2981631e-03, -3.2240836e-04,  9.3357603e-04,\n",
      "       -2.7423288e-04,  7.3373172e-04,  2.9039395e-05, -7.6650213e-05,\n",
      "        1.1548300e-03,  0.0000000e+00,  1.6888443e-03, -3.3782018e-04,\n",
      "       -3.5455372e-04, -1.7258030e-03, -2.9158834e-04,  6.6835397e-05,\n",
      "       -3.8211056e-04, -3.2591682e-03, -2.0579100e-03, -2.2203748e-03,\n",
      "        5.5937502e-03, -2.0223897e-04, -9.5696974e-04,  0.0000000e+00,\n",
      "       -1.9010113e-03, -9.7753666e-04, -2.4153318e-03,  3.9381423e-04,\n",
      "       -3.5435313e-04, -4.8823745e-04,  1.2461091e-03,  2.6479847e-04,\n",
      "       -3.6262947e-03, -1.3936014e-03,  4.9985026e-04, -2.1489653e-04,\n",
      "       -1.5662838e-03,  6.0878007e-04, -8.4725645e-04, -7.9268293e-04,\n",
      "        1.1829508e-04, -5.4255628e-04,  0.0000000e+00, -1.8143536e-03,\n",
      "       -1.7446656e-03,  0.0000000e+00,  0.0000000e+00,  1.7092936e-03,\n",
      "       -1.0199915e-03, -1.5738221e-03,  1.3662176e-04, -8.3762442e-04],\n",
      "      dtype=float32)>, <tf.Variable 'neural__network/dense_2/kernel:0' shape=(128, 2) dtype=float32, numpy=\n",
      "array([[ 1.94586188e-01, -6.89369962e-02],\n",
      "       [-1.55388534e-01,  2.59983689e-02],\n",
      "       [ 1.34127930e-01,  1.38365209e-01],\n",
      "       [ 1.45033086e-02, -7.77772367e-02],\n",
      "       [ 1.23189524e-01, -1.87102839e-01],\n",
      "       [-1.23468027e-01,  1.99319646e-01],\n",
      "       [ 1.77167431e-01, -2.42943585e-01],\n",
      "       [ 1.31157681e-01, -2.73030996e-03],\n",
      "       [ 1.86585542e-02,  3.20565775e-02],\n",
      "       [-2.34084949e-01,  2.92496216e-02],\n",
      "       [-1.22731768e-01,  9.89669859e-02],\n",
      "       [ 1.07565515e-01, -1.24803998e-01],\n",
      "       [ 1.34203821e-01, -1.80664778e-01],\n",
      "       [ 3.21797818e-01, -2.86422580e-01],\n",
      "       [ 1.99884757e-01,  5.93125112e-02],\n",
      "       [ 5.66381253e-02,  1.18897706e-01],\n",
      "       [ 4.10825238e-02,  1.56974830e-02],\n",
      "       [ 1.43825457e-01,  7.50499666e-02],\n",
      "       [-1.38045058e-01,  2.03132436e-01],\n",
      "       [-2.10944951e-01, -4.00924645e-02],\n",
      "       [ 3.25483717e-02,  2.83097446e-01],\n",
      "       [ 7.49049485e-02, -2.39304453e-02],\n",
      "       [-1.69867381e-01, -2.92884186e-02],\n",
      "       [-6.22180700e-02,  2.54075944e-01],\n",
      "       [ 7.47936144e-02, -1.12740621e-01],\n",
      "       [-9.30001885e-02, -2.17061043e-01],\n",
      "       [ 7.94312805e-02, -4.94380593e-02],\n",
      "       [ 1.17125943e-01, -1.79860279e-01],\n",
      "       [ 2.29226992e-01, -5.56191877e-02],\n",
      "       [-1.24332443e-01,  2.34281600e-01],\n",
      "       [-1.66757107e-02, -2.51083821e-01],\n",
      "       [ 3.19197923e-02, -2.13413000e-01],\n",
      "       [ 2.58300275e-01,  1.61112919e-01],\n",
      "       [ 2.54797991e-02,  8.09655115e-02],\n",
      "       [-2.47139528e-01, -4.44956385e-02],\n",
      "       [-3.74820203e-01,  1.93833709e-01],\n",
      "       [ 7.96134621e-02,  3.86391096e-02],\n",
      "       [ 1.84235096e-01, -2.94342414e-02],\n",
      "       [ 1.07631974e-01,  1.27787488e-02],\n",
      "       [ 1.18578754e-01, -4.18776572e-02],\n",
      "       [ 1.25743896e-01, -1.82058945e-01],\n",
      "       [-1.78664878e-01,  2.23420963e-01],\n",
      "       [ 1.22664096e-02, -1.02993838e-01],\n",
      "       [-1.72624215e-01,  1.51699051e-01],\n",
      "       [ 1.70925692e-01, -2.28269339e-01],\n",
      "       [-3.46504956e-01,  2.65941530e-01],\n",
      "       [ 4.51230891e-02,  1.44730091e-01],\n",
      "       [-1.09928422e-01, -1.28491610e-01],\n",
      "       [-3.98026407e-02,  2.14449704e-01],\n",
      "       [-2.06354693e-01, -1.28861681e-01],\n",
      "       [-1.90268099e-01,  2.11000264e-01],\n",
      "       [ 3.44027787e-01, -3.80138934e-01],\n",
      "       [ 1.60577357e-01,  1.75697535e-01],\n",
      "       [-1.40884310e-01,  1.04365259e-01],\n",
      "       [ 1.56341627e-01, -1.68957308e-01],\n",
      "       [ 7.89945275e-02, -2.67041296e-01],\n",
      "       [-2.63592284e-02,  7.85880163e-02],\n",
      "       [ 9.87789780e-02,  2.59153843e-02],\n",
      "       [ 1.86373651e-01, -2.92625248e-01],\n",
      "       [-1.03618406e-01,  7.13119432e-02],\n",
      "       [-1.72576889e-01, -9.32566151e-02],\n",
      "       [ 2.14383397e-02,  5.07153524e-03],\n",
      "       [-1.44049004e-01,  8.12496617e-02],\n",
      "       [ 8.75131935e-02, -1.89761013e-01],\n",
      "       [ 2.27928728e-01, -1.17054418e-01],\n",
      "       [-7.43714646e-02,  2.65426040e-01],\n",
      "       [-1.68428287e-01,  2.56091386e-01],\n",
      "       [-2.50351727e-01,  2.39328787e-01],\n",
      "       [ 1.92330733e-01, -3.13910918e-04],\n",
      "       [ 6.49021240e-03,  1.47729144e-01],\n",
      "       [ 6.63044006e-02, -1.40339658e-01],\n",
      "       [-5.07519487e-03, -5.25307171e-02],\n",
      "       [ 2.04994500e-01,  2.86107864e-02],\n",
      "       [-4.27839421e-02, -2.52228647e-01],\n",
      "       [ 7.98902139e-02, -2.83027321e-01],\n",
      "       [-2.94510275e-01,  5.62322140e-02],\n",
      "       [ 2.62276173e-01, -2.14206412e-01],\n",
      "       [-1.78269506e-01,  1.28383022e-02],\n",
      "       [ 2.98023522e-02, -1.98638469e-01],\n",
      "       [-2.09308211e-02, -2.79930621e-01],\n",
      "       [ 1.70490712e-01, -2.42045179e-01],\n",
      "       [ 1.08297981e-01,  3.01272422e-01],\n",
      "       [ 4.72010113e-02,  5.79681695e-02],\n",
      "       [-4.04340355e-03,  2.05382658e-03],\n",
      "       [-1.75178312e-02,  2.91410238e-01],\n",
      "       [ 1.36471406e-01, -2.02657714e-01],\n",
      "       [ 2.46900693e-01, -2.53674507e-01],\n",
      "       [-2.42672756e-01,  2.79178917e-01],\n",
      "       [-1.00717202e-01, -1.94819659e-01],\n",
      "       [-2.65123516e-01, -1.13079809e-01],\n",
      "       [-3.43052119e-01, -3.35958786e-02],\n",
      "       [ 5.54840006e-02,  4.13558353e-03],\n",
      "       [-1.97897792e-01, -2.58803908e-02],\n",
      "       [ 1.91210374e-01, -1.50263309e-01],\n",
      "       [-1.60419434e-01,  1.06920205e-01],\n",
      "       [-1.65083990e-01,  2.38711208e-01],\n",
      "       [ 2.95074105e-01, -2.96832949e-01],\n",
      "       [-1.28043607e-01,  1.74970865e-01],\n",
      "       [-1.97696730e-01,  5.19893924e-03],\n",
      "       [ 3.90289575e-02,  1.19806185e-01],\n",
      "       [ 6.45998940e-02, -3.10508776e-02],\n",
      "       [-2.51424849e-01, -2.93838363e-02],\n",
      "       [ 1.10758908e-01, -1.67612642e-01],\n",
      "       [ 1.55521676e-01,  1.85052827e-01],\n",
      "       [-1.68903813e-01, -1.14322357e-01],\n",
      "       [-1.44907981e-01, -1.91944018e-01],\n",
      "       [-9.80781466e-02,  5.08198477e-02],\n",
      "       [-2.16730982e-01,  3.42760682e-02],\n",
      "       [-3.63655210e-01,  2.17315897e-01],\n",
      "       [-9.09277238e-03,  2.08864138e-01],\n",
      "       [-1.25036910e-01,  1.37822390e-01],\n",
      "       [ 7.36807957e-02, -1.26568317e-01],\n",
      "       [-6.57913461e-02,  2.57453412e-01],\n",
      "       [ 1.98083267e-01, -1.18875787e-01],\n",
      "       [-1.66923419e-01, -3.22189182e-02],\n",
      "       [ 2.52232462e-01, -8.56708586e-02],\n",
      "       [-1.57939613e-01,  9.46780592e-02],\n",
      "       [-2.03718171e-01,  8.15416351e-02],\n",
      "       [ 4.07009572e-02,  9.89471525e-02],\n",
      "       [-1.97783425e-01,  1.34670421e-01],\n",
      "       [ 2.23011777e-01, -1.51750788e-01],\n",
      "       [-4.97630537e-02, -1.18448444e-01],\n",
      "       [-1.21612750e-01, -2.95095295e-02],\n",
      "       [ 4.34607804e-01, -4.61661994e-01],\n",
      "       [ 1.14675164e-01, -1.99322835e-01],\n",
      "       [ 1.55539557e-01, -1.25248209e-01],\n",
      "       [-3.09922583e-02,  2.16582105e-01],\n",
      "       [ 1.80384934e-01,  8.72422382e-02]], dtype=float32)>, <tf.Variable 'neural__network/dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.0023932 , -0.00239319], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()\n",
    "print(new_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(X, np.float32)\n",
    "pred = new_model(x, is_training = False)\n",
    "print(accuracy_NN(pred, Y))\n",
    "T = pd.read_csv(\"test_data.csv\", index_col = False)\n",
    "x_t = T.drop('Class', axis = 1)\n",
    "x_t = np.array(x_t, np.float32)\n",
    "y_t = T['Class']\n",
    "pred = new_model(x_t, is_training = False)\n",
    "print(accuracy_NN(pred, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = tf.keras.models.load_model('test_NN_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "dense_weight = np.asarray(neural_network.trainable_variables[0])\n",
    "dense_bias = np.asarray(neural_network.trainable_variables[1])\n",
    "dense1_weight = np.asarray(neural_network.trainable_variables[2])\n",
    "dense1_bias = np.asarray(neural_network.trainable_variables[3])\n",
    "dense2_weight = np.asarray(neural_network.trainable_variables[4])\n",
    "dense2_bias = np.asarray(neural_network.trainable_variables[5])\n",
    "# np.savetxt('trainable_variables/dense_weight_50x256.csv', np.transpose(dense_weight), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense_bias_256.csv', np.transpose(dense_bias), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense1_weight_256x128.csv', np.transpose(dense1_weight), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense1_bias_128.csv', np.transpose(dense1_bias), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense2_weight_128x2.csv', np.transpose(dense2_weight), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense2_bias_2.csv', np.transpose(dense2_bias), delimiter=',')\n",
    "np.savetxt('trainable_variables/dense_weight_50x256.csv', dense_weight, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense_bias_256.csv', dense_bias, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense1_weight_256x128.csv', dense1_weight, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense1_bias_128.csv', dense1_bias, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense2_weight_128x2.csv', dense2_weight, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense2_bias_2.csv', dense2_bias, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
