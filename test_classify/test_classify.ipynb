{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import random\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"test_classification_data_with_date.csv\", index_col = False)\n",
    "df = pd.read_csv(\"test_classification_data.csv\", index_col = False)\n",
    "# df = df.drop(df[(df.Class == 'H')].index).drop('date', axis = 1)\n",
    "df = df.drop('date', axis = 1)\n",
    "X = df.drop('Class', axis = 1)\n",
    "Y = df['Class']\n",
    "\n",
    "kf = KFold(n_splits = 3)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "plt.figure(figsize=(80,15))\n",
    "for i in range (X.shape[0]):\n",
    "    if Y.iloc[i] == 2:\n",
    "        plt.plot(X.iloc[i], color = 'r')\n",
    "    if Y.iloc[i] == 1:\n",
    "        plt.plot(X.iloc[i], color = 'b')\n",
    "    if Y.iloc[i] == 0:\n",
    "        plt.plot(X.iloc[i], color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, 15):\n",
    "    print(\"kernel: poly, degree: \", i)\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        SVClassifier = SVC(kernel = 'poly', degree = i)\n",
    "        SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "        y_train_pred = SVClassifier.predict(X_train)\n",
    "        y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    #     print(confusion_matrix(y_train_pred, Y_train))\n",
    "    #     print(classification_report(y_train_pred, Y_train))\n",
    "    #     print(confusion_matrix(y_test_pred, Y_test))\n",
    "        train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "        test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "    print(\"average training accuracy: \", train_acc / 5)\n",
    "    print(\"average testing accuracy: \", test_acc / 5)\n",
    "    print()\n",
    "\n",
    "print(\"kernel: linear\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    SVClassifier = SVC(kernel = 'linear')\n",
    "    SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "    y_train_pred = SVClassifier.predict(X_train)\n",
    "    y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()\n",
    "\n",
    "print(\"kernel: rbf\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    SVClassifier = SVC()\n",
    "    SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "    y_train_pred = SVClassifier.predict(X_train)\n",
    "    y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()\n",
    "\n",
    "print(\"kernel: sigmoid\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    SVClassifier = SVC(kernel = 'sigmoid')\n",
    "    SVClassifier.fit(X_train, Y_train)\n",
    "\n",
    "    y_train_pred = SVClassifier.predict(X_train)\n",
    "    y_test_pred = SVClassifier.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(y_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(y_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    logreg_clf = LogisticRegression()\n",
    "    logreg_clf.fit(X_train, Y_train)\n",
    "    logreg_train_pred = logreg_clf.predict(X_train)\n",
    "    logreg_test_pred = logreg_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(logreg_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(logreg_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    tree_clf = DecisionTreeClassifier()\n",
    "    tree_clf.fit(X_train, Y_train)\n",
    "    tree_train_pred = tree_clf.predict(X_train)\n",
    "    tree_test_pred = tree_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(tree_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(tree_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"K Nearest Neighbor\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    knn_clf.fit(X_train, Y_train)\n",
    "    knn_train_pred = knn_clf.predict(X_train)\n",
    "    knn_test_pred = knn_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(knn_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(knn_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Discriminant Analysis\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    lda_clf.fit(X_train, Y_train)\n",
    "    lda_train_pred = lda_clf.predict(X_train)\n",
    "    lda_test_pred = lda_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(lda_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(lda_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gaussian Naive Bayes\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf.fit(X_train, Y_train)\n",
    "    gnb_train_pred = gnb_clf.predict(X_train)\n",
    "    gnb_test_pred = gnb_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(gnb_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(gnb_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest - gini\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    rf_clf = RandomForestClassifier()\n",
    "    rf_clf.fit(X_train, Y_train)\n",
    "    rf_train_pred = rf_clf.predict(X_train)\n",
    "    rf_test_pred = rf_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(rf_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(rf_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest - entropy\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    rf2_clf = RandomForestClassifier(criterion = 'entropy')\n",
    "    rf2_clf.fit(X_train, Y_train)\n",
    "    rf2_train_pred = rf2_clf.predict(X_train)\n",
    "    rf2_test_pred = rf2_clf.predict(X_test)\n",
    "\n",
    "    train_acc += accuracy_score(rf2_train_pred, Y_train)\n",
    "    test_acc += accuracy_score(rf2_test_pred, Y_test)\n",
    "print(\"average training accuracy: \", train_acc / 5)\n",
    "print(\"average testing accuracy: \", test_acc / 5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# MNIST dataset parameters\n",
    "num_classes = 2\n",
    "num_features = 50\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 20000\n",
    "batch_size = 256\n",
    "display_steps = 250\n",
    "\n",
    "# load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(20000).batch(batch_size).prefetch(1)\n",
    "\n",
    "# Weight of shape [784, 10], the 28*28 image features, and total number of classes.\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "# Bias of shape [10], the total number of classes.\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
    "\n",
    "# logistic regression (Wx + b)\n",
    "def logistic_regression(x):\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    # ****************\n",
    "    # softmax is a generalization of sigmoid function\n",
    "    # sigmoid function is specifiically for binary classification\n",
    "    # ****************\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy_LogReg(y_pred, y_true):\n",
    "    # Encode label to a one hot vector.\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    # Compute cross-entropy.\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred),1))\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy_LogReg(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "def run_optimization_LogReg(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = logistic_regression(x)\n",
    "        loss = cross_entropy_LogReg(pred, y)\n",
    "        \n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate (train_data.take(training_steps), 1):\n",
    "    run_optimization_LogReg(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_steps == 0:\n",
    "        pred = logistic_regression(batch_x)\n",
    "        loss = cross_entropy_LogReg(pred, batch_y)\n",
    "        acc = accuracy_LogReg(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "        \n",
    "# Test model on validation set.\n",
    "pred = logistic_regression(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy_LogReg(pred, y_test))\n",
    "\n",
    "# # Predict 5 images from validation set.\n",
    "# n_images = 5\n",
    "# test_images = random.choices(x_test, k = n_images)\n",
    "# predictions = logistic_regression(test_images)\n",
    "\n",
    "# # Display image and model prediction.\n",
    "# for i in range(n_images):\n",
    "# #     plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "# #     plt.show()\n",
    "#     print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, loss: 0.784175, accuracy: 0.625000\n",
      "step: 250, loss: 0.075977, accuracy: 1.000000\n",
      "step: 500, loss: 0.025975, accuracy: 1.000000\n",
      "step: 750, loss: 0.013356, accuracy: 1.000000\n",
      "step: 1000, loss: 0.009196, accuracy: 1.000000\n",
      "step: 1250, loss: 0.006428, accuracy: 1.000000\n",
      "step: 1500, loss: 0.005051, accuracy: 1.000000\n",
      "step: 1750, loss: 0.003810, accuracy: 1.000000\n",
      "step: 2000, loss: 0.003334, accuracy: 1.000000\n",
      "Test Accuracy: 0.769231\n",
      "step: 5, loss: 0.065814, accuracy: 0.960938\n",
      "step: 250, loss: 0.004413, accuracy: 1.000000\n",
      "step: 500, loss: 0.002958, accuracy: 1.000000\n",
      "step: 750, loss: 0.002147, accuracy: 1.000000\n",
      "step: 1000, loss: 0.001552, accuracy: 1.000000\n",
      "step: 1250, loss: 0.001713, accuracy: 1.000000\n",
      "step: 1500, loss: 0.001179, accuracy: 1.000000\n",
      "step: 1750, loss: 0.000985, accuracy: 1.000000\n",
      "step: 2000, loss: 0.000855, accuracy: 1.000000\n",
      "Test Accuracy: 0.692308\n",
      "step: 5, loss: 1.592554, accuracy: 0.593750\n",
      "step: 250, loss: 0.006552, accuracy: 1.000000\n",
      "step: 500, loss: 0.004533, accuracy: 1.000000\n",
      "step: 750, loss: 0.003328, accuracy: 1.000000\n",
      "step: 1000, loss: 0.002809, accuracy: 1.000000\n",
      "step: 1250, loss: 0.002191, accuracy: 1.000000\n",
      "step: 1500, loss: 0.001820, accuracy: 1.000000\n",
      "step: 1750, loss: 0.001847, accuracy: 1.000000\n",
      "step: 2000, loss: 0.001537, accuracy: 1.000000\n",
      "Test Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "# MNIST dataset parameters\n",
    "num_classes = 2\n",
    "num_features = 50\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 2000\n",
    "batch_size = 256\n",
    "display_steps = 250\n",
    "\n",
    "# Network parameters\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 128\n",
    "\n",
    "# load dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "# x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# # x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# # x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "# # Use tf.data API to shuffle and batch data.\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_data = train_data.repeat().shuffle(20000).batch(batch_size).prefetch(1)\n",
    "\n",
    "# Create NN model\n",
    "class Neural_Network(Model):\n",
    "    # Set layers\n",
    "    def __init__(self):\n",
    "        tf.keras.backend.clear_session()\n",
    "        super(Neural_Network, self).__init__();\n",
    "        # 1st fully connected hidden layer\n",
    "        self.fc1 = layers.Dense(n_hidden_1, activation = tf.nn.relu)\n",
    "        # 2nd fully connected hidden layer\n",
    "        self.fc2 = layers.Dense(n_hidden_2, activation = tf.nn.relu)\n",
    "        # Output layer\n",
    "        self.out = layers.Dense(num_classes)\n",
    "        \n",
    "    # Set forward pass\n",
    "    def call(self, x, is_training = False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build NN model\n",
    "neural_network = Neural_Network()\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy_NN(y_pred, y_true):\n",
    "    # Convert labels to int64 for tf cross-entropy function\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n",
    "    # Average loss across batch\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy_NN(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis = -1)\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Optimization process\n",
    "def run_optimization_NN(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = neural_network(x, is_training = True)\n",
    "        loss = cross_entropy_NN(pred, y)\n",
    "        \n",
    "    trainable_variables = neural_network.trainable_variables\n",
    "    \n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "#     x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "#     x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "    # Use tf.data API to shuffle and batch data.\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.repeat().shuffle(20000).batch(batch_size).prefetch(1)\n",
    "    for step, (batch_x, batch_y) in enumerate (train_data.take(training_steps), 1):\n",
    "        run_optimization_NN(batch_x, batch_y)\n",
    "\n",
    "        if step % display_steps == 0 or step == 5:\n",
    "            pred = neural_network(batch_x, is_training = True)\n",
    "            loss = cross_entropy_NN(pred, batch_y)\n",
    "            acc = accuracy_NN(pred, batch_y)\n",
    "            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "        \n",
    "    # Test model on validation set.\n",
    "    pred = neural_network(x_test, is_training = False)\n",
    "    print(\"Test Accuracy: %f\" % accuracy_NN(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# print(x_train)\n",
    "# print(x_test.shape)\n",
    "# print(X)\n",
    "# print(Y)\n",
    "# print(y_train)\n",
    "x = np.array(X, np.float32)\n",
    "pred = neural_network(x, is_training = False)\n",
    "print(accuracy_NN(pred, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.9285714, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.0000000e+00 9.1769161e-09]\n",
      " [9.9924797e-01 7.5200544e-04]\n",
      " [1.0000000e+00 2.4831050e-11]\n",
      " [1.0000000e+00 1.2341097e-08]\n",
      " [1.0000000e+00 5.6916601e-08]\n",
      " [9.9999928e-01 7.7428854e-07]\n",
      " [9.9999797e-01 2.0296154e-06]\n",
      " [9.9999487e-01 5.0663712e-06]\n",
      " [9.9961346e-01 3.8653036e-04]\n",
      " [3.9801882e-03 9.9601978e-01]\n",
      " [3.9227560e-01 6.0772437e-01]\n",
      " [7.9714311e-03 9.9202853e-01]\n",
      " [1.0000000e+00 1.5720708e-09]\n",
      " [1.0000000e+00 2.7164451e-09]], shape=(14, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "T = pd.read_csv(\"test_data.csv\", index_col = False)\n",
    "T = T.drop('Date', axis = 1)\n",
    "x_t = T.drop('Class', axis = 1)\n",
    "x_t = np.array(x_t, np.float32)\n",
    "y_t = T['Class']\n",
    "pred = neural_network(x_t, is_training = False)\n",
    "print(accuracy_NN(pred, y_t))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.save_model(neural_network, \"test_NN_1.0.h5\")\n",
    "neural_network.save('test_NN_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('test_NN_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()\n",
    "print(new_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(X, np.float32)\n",
    "pred = new_model(x, is_training = False)\n",
    "print(accuracy_NN(pred, Y))\n",
    "T = pd.read_csv(\"test_data.csv\", index_col = False)\n",
    "x_t = T.drop('Class', axis = 1)\n",
    "x_t = np.array(x_t, np.float32)\n",
    "y_t = T['Class']\n",
    "pred = new_model(x_t, is_training = False)\n",
    "print(accuracy_NN(pred, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = tf.keras.models.load_model('test_NN_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "dense_weight = np.asarray(neural_network.trainable_variables[0])\n",
    "dense_bias = np.asarray(neural_network.trainable_variables[1])\n",
    "dense1_weight = np.asarray(neural_network.trainable_variables[2])\n",
    "dense1_bias = np.asarray(neural_network.trainable_variables[3])\n",
    "dense2_weight = np.asarray(neural_network.trainable_variables[4])\n",
    "dense2_bias = np.asarray(neural_network.trainable_variables[5])\n",
    "# np.savetxt('trainable_variables/dense_weight_50x256.csv', np.transpose(dense_weight), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense_bias_256.csv', np.transpose(dense_bias), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense1_weight_256x128.csv', np.transpose(dense1_weight), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense1_bias_128.csv', np.transpose(dense1_bias), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense2_weight_128x2.csv', np.transpose(dense2_weight), delimiter=',')\n",
    "# np.savetxt('trainable_variables/dense2_bias_2.csv', np.transpose(dense2_bias), delimiter=',')\n",
    "np.savetxt('trainable_variables/dense_weight_50x256.csv', dense_weight, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense_bias_256.csv', dense_bias, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense1_weight_256x128.csv', dense1_weight, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense1_bias_128.csv', dense1_bias, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense2_weight_128x2.csv', dense2_weight, delimiter=',')\n",
    "np.savetxt('trainable_variables/dense2_bias_2.csv', dense2_bias, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
