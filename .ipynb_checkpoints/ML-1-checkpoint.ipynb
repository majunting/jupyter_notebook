{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import Model, layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns = ['image', 'cells'])\n",
    "\n",
    "# # base_dir = os.path.join(os.path.abspath(''), \"Aria1/\")\n",
    "# base_dir = \"../../20201020134002/Aria1/\"\n",
    "# img_prefix = \"Area1_Cell_\"\n",
    "# img_suffix = \"_bin.png\"\n",
    "\n",
    "# directory = os.fsencode(base_dir)\n",
    "# for file in os.listdir(directory):\n",
    "#     filename = os.fsdecode(file)\n",
    "#     if filename.endswith(img_suffix):\n",
    "#         df = df.append({'image': filename, 'cells': \"\"}, ignore_index = True)\n",
    "# df.to_csv(r'../../20201020134002/dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../20201020134002/dataset_partial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = data[data.columns[0]]\n",
    "# for i in range (0, 400):\n",
    "#     print(i)\n",
    "\n",
    "# imagepaths, cells = list(), list()\n",
    "# img_list = data[data.columns[0]]\n",
    "# cell_list = data[data.columns[1]]\n",
    "# for idx in range (0, 400):\n",
    "#     imagepaths.append(\"./Aria1/\" + img_list[idx])\n",
    "#     cells.append(int(cell_list[idx]))\n",
    "# print(imagepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image parameters\n",
    "# Channels = 3\n",
    "\n",
    "# base_dir = \"../../20201020134002/\"\n",
    "\n",
    "# def read_images(dataset_dir, batch_size):\n",
    "#     imagepaths, cells = list(), list()\n",
    "#     data = pd.read_csv(dataset_dir)\n",
    "#     img_list = data[data.columns[0]]\n",
    "#     cell_list = data[data.columns[1]]\n",
    "#     for idx in range (0, 400):\n",
    "#         imagepaths.append(base_dir + \"./Aria1/\" + img_list[idx])\n",
    "#         cells.append(str(cell_list[idx]))\n",
    "    \n",
    "#     imagepaths = tf.convert_to_tensor(imagepaths, dtype = tf.string)\n",
    "#     cells = tf.convert_to_tensor(cells, dtype = tf.string)\n",
    "#     image, cell = tf.data.Dataset.from_tensor_slices([imagepaths, cells])\n",
    "    \n",
    "#     image = tf.io.read_file(image)\n",
    "#     image = tf.image.decode_jpeg(image, channels = Channels)\n",
    "    \n",
    "#     X, Y = tf.train.batch([image, cell], batch_size = batch_size, capacity = batch_size * 8, num_threads = 4)\n",
    "    \n",
    "#     return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------\n",
    "# # THIS IS A CLASSIC CNN (see examples, section 3)\n",
    "# # -----------------------------------------------\n",
    "# # Note that a few elements have changed (usage of queues).\n",
    "\n",
    "# # Parameters\n",
    "# learning_rate = 0.001\n",
    "# num_steps = 10000\n",
    "# batch_size = 128\n",
    "# display_step = 100\n",
    "\n",
    "# # Network Parameters\n",
    "# dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# DATASET_PATH = base_dir + \"dataset.csv\"\n",
    "\n",
    "# # Build the data input\n",
    "# X, Y = read_images(DATASET_PATH, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../../20201020134002/\"\n",
    "\n",
    "with open(base_dir + \"dataset_partial.csv\") as f:\n",
    "    dataset_file = f.read().splitlines()\n",
    "\n",
    "# Load the whole dataset file, and slice each line.\n",
    "data = tf.data.Dataset.from_tensor_slices(dataset_file)\n",
    "# # Refill data indefinitely.\n",
    "data = data.repeat()\n",
    "# Shuffle data.\n",
    "data = data.shuffle(buffer_size=1000)\n",
    "\n",
    "# Load and pre-process images.\n",
    "def load_image(path):\n",
    "    # Read image from path.\n",
    "    image = tf.io.read_file(path)\n",
    "    # Decode the jpeg image to array [0, 255].\n",
    "    image = tf.image.decode_jpeg(image, channels = 3)\n",
    "    # Resize images to a common size of 256x256.\n",
    "    image = tf.image.resize(image, [28, 28])\n",
    "    # Rescale values to [-1, 1].\n",
    "    image = 1. - image / 127.5\n",
    "    return image\n",
    "# Decode each line from the dataset file.\n",
    "def parse_records(line):\n",
    "    # File is in csv format: \"image_path,label_id\".\n",
    "    # TensorFlow requires a default value, but it will never be used.\n",
    "    image_path, image_label = tf.io.decode_csv(line, [\"\", 0])\n",
    "    # Apply the function to load images.\n",
    "    image = load_image(base_dir + \"/Aria1/\" + image_path)\n",
    "    return image, image_label\n",
    "# Use 'map' to apply the above functions in parallel.\n",
    "data = data.map(parse_records, num_parallel_calls=4)\n",
    "\n",
    "# Batch data (aggregate images-array together).\n",
    "data = data.batch(batch_size=128)\n",
    "# Prefetch batch (pre-load batch for faster consumption).\n",
    "data = data.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch_x, batch_y in data.take(1):\n",
    "#     print(batch_x)\n",
    "#     print(\"------------------------------------\")\n",
    "#     print(batch_y)\n",
    "\n",
    "# print(len(data))\n",
    "# print(data)\n",
    "\n",
    "# test_data.batch(batch_size = test_size)\n",
    "# x_test= test_data.take(1)\n",
    "# print(x_test.shape)\n",
    "# # print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATASET_SIZE = len(data)\n",
    "# train_size = int(0.8 * DATASET_SIZE)\n",
    "# # val_size = int(0.15 * DATASET_SIZE)\n",
    "# test_size = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "# train_data = data.take(train_size)\n",
    "# test_data = data.skip(train_size)\n",
    "# # val_data = test_data.skip(test_size)\n",
    "# test_data = test_data.take(test_size)\n",
    "\n",
    "# train_data = train_data.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 250, loss: 0.868328, accuracy: 0.710938\n",
      "step: 500, loss: 0.665528, accuracy: 0.796875\n",
      "step: 750, loss: 0.681540, accuracy: 0.812500\n",
      "step: 1000, loss: 0.713351, accuracy: 0.773438\n",
      "step: 1250, loss: 0.489167, accuracy: 0.859375\n",
      "step: 1500, loss: 0.553705, accuracy: 0.828125\n",
      "step: 1750, loss: 0.645251, accuracy: 0.835938\n",
      "step: 2000, loss: 0.642763, accuracy: 0.828125\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset parameters\n",
    "num_classes = 10\n",
    "num_features = 784\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 2000\n",
    "batch_size = 256\n",
    "display_steps = 250\n",
    "\n",
    "# Network Parameters\n",
    "conv1_filter = 32\n",
    "conv2_filter = 64\n",
    "fc_units = 1024\n",
    "\n",
    "# Create CNN model\n",
    "class Convolutional_Network(Model):\n",
    "    # Set layers\n",
    "    def __init__(self):\n",
    "        super(Convolutional_Network, self).__init__()\n",
    "        # 1st convolutional layer\n",
    "        self.conv1 = layers.Conv2D(conv1_filter, kernel_size = 5, activation = tf.nn.relu)\n",
    "        # 1st max pooling layer\n",
    "        self.maxpool1 = layers.MaxPool2D(pool_size = 2, strides = 2)\n",
    "        # 2nd convolutional layer\n",
    "        self.conv2 = layers.Conv2D(conv2_filter, kernel_size = 3, activation = tf.nn.relu)\n",
    "        # 2nd max pooling layer\n",
    "        self.maxpool2 = layers.MaxPool2D(pool_size = 2, strides = 2)\n",
    "        \n",
    "        # Flatten data to 1-D vector for the fully connected layer\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = layers.Dense(fc_units)\n",
    "        # Apply dropout (if is_training is False, dropout is not applied)\n",
    "        self.dropout = layers.Dropout(rate = 0.5)\n",
    "        # Output layer\n",
    "        self.out = layers.Dense(num_classes)\n",
    "    \n",
    "    # Set forward pass\n",
    "    def call(self, x, is_training = False):\n",
    "#         x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x, training = is_training)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build CNN model\n",
    "convolutional_network = Convolutional_Network()\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy_CNN(y_pred, y_true):\n",
    "    # Convert labels to int64 for tf cross-entropy function\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n",
    "    # Average loss across batch\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy_CNN(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis = -1)\n",
    "\n",
    "# Adam gradient descent optimizer.\n",
    "# *******************\n",
    "# SGD gives slightly worse results, change to Adam\n",
    "# *******************\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Optimization process\n",
    "def run_optimization_CNN(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = convolutional_network(x, is_training = True)\n",
    "        loss = cross_entropy_CNN(pred, y)\n",
    "        \n",
    "    trainable_variables = convolutional_network.trainable_variables\n",
    "    \n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate (data.take(training_steps), 1):\n",
    "    run_optimization_CNN(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_steps == 0:\n",
    "        pred = convolutional_network(batch_x, is_training = True)\n",
    "        loss = cross_entropy_CNN(pred, batch_y)\n",
    "        acc = accuracy_CNN(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Test model on validation set.\n",
    "# pred = convolutional_network(x_test, is_training = False)\n",
    "# print(\"Test Accuracy: %f\" % accuracy_CNN(pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
